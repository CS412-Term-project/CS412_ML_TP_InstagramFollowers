{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcRTuqJFkZVY"
      },
      "outputs": [],
      "source": [
        "# randomized search\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import string\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "!pip install stanza\n",
        "import stanza\n",
        "\n",
        "# For RandomizedSearchCV parameter sampling\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize Stanza for Turkish\n",
        "stanza.download('tr')\n",
        "nlp = stanza.Pipeline('tr', processors='tokenize', tokenize_no_ssplit=True)\n",
        "\n",
        "# Load the Turkish stopwords\n",
        "turkish_stopwords = set(stopwords.words('turkish'))\n",
        "\n",
        "# File paths\n",
        "labels_path = \"/content/drive/My Drive/Colab Notebooks/train-classification.csv\"\n",
        "features_path = \"/content/drive/My Drive/Colab Notebooks/training-dataset.jsonl\"\n",
        "test_path = \"/content/drive/My Drive/Colab Notebooks/test-classification-round3.dat\"\n",
        "\n",
        "# 1. Load Features\n",
        "features = []\n",
        "with open(features_path, 'rt', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        features.append(json.loads(line))\n",
        "features_df = pd.DataFrame(features)\n",
        "print(\"Features loaded.\")\n",
        "\n",
        "# 2. Load Labels\n",
        "labels_df = pd.read_csv(labels_path)\n",
        "\n",
        "# Map categories to numbers\n",
        "category_mapping = {\n",
        "    'Mom and Children': 0,\n",
        "    'Food': 1,\n",
        "    'Travel': 2,\n",
        "    'Gaming': 3,\n",
        "    'Tech': 4,\n",
        "    'Health and Lifestyle': 5,\n",
        "    'Fashion': 6,\n",
        "    'Sports': 7,\n",
        "    'Entertainment': 8,\n",
        "    'Art': 9\n",
        "}\n",
        "labels_df['label'] = labels_df['label'].map(category_mapping)\n",
        "\n",
        "# Drop NaN labels\n",
        "labels_df = labels_df.dropna(subset=['label'])\n",
        "print(\"Labels loaded and mapped.\")\n",
        "\n",
        "def preprocess_caption(caption):\n",
        "    # Normalize Unicode characters\n",
        "    caption = unicodedata.normalize('NFKC', caption)\n",
        "    caption = caption.casefold()\n",
        "    # Remove URLs\n",
        "    caption = re.sub(r'http\\S+|www\\S+|https\\S+', '', caption, flags=re.MULTILINE)\n",
        "    # Remove special characters and punctuation\n",
        "    caption = re.sub(r'[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s#@]', '', caption)\n",
        "    # Remove numbers\n",
        "    caption = re.sub(r'\\d+', '', caption)\n",
        "    # Remove extra whitespaces\n",
        "    caption = re.sub(r'\\s+', ' ', caption).strip()\n",
        "    return caption\n",
        "\n",
        "def count_food_words_exact(posts, food_words):\n",
        "    \"\"\"\n",
        "    Counts the number of food-related words in all captions of a user's posts using exact word matching.\n",
        "    \"\"\"\n",
        "    if not isinstance(posts, list):\n",
        "        return 0  # Return 0 if posts is not a list\n",
        "\n",
        "    count = 0\n",
        "    for post in posts:\n",
        "        caption = post.get('caption', '')\n",
        "        if not isinstance(caption, str):\n",
        "            continue  # Skip if caption is not a string\n",
        "\n",
        "        caption = preprocess_caption(caption)\n",
        "        doc = nlp(caption)\n",
        "        words = [word.text for sent in doc.sentences for word in sent.words]\n",
        "        # Optionally remove stopwords\n",
        "        words = [word for word in words if word not in turkish_stopwords]\n",
        "\n",
        "        count += sum(1 for word in words if word in food_words)\n",
        "\n",
        "    return count\n",
        "\n",
        "# 3. Merge Features and Labels\n",
        "features_df['username'] = features_df['profile'].apply(lambda x: x['username'])\n",
        "merged_data = features_df.merge(labels_df, left_on='username', right_on='Unnamed: 0', how='inner')\n",
        "\n",
        "# 4. Feature Extraction\n",
        "merged_data['followers_count'] = merged_data['profile'].apply(lambda x: x.get('followers_count', 0))\n",
        "merged_data['following_count'] = merged_data['profile'].apply(lambda x: x.get('following_count', 0))\n",
        "merged_data['media_count'] = merged_data['profile'].apply(lambda x: x.get('media_count', 0))\n",
        "merged_data['num_posts'] = merged_data['posts'].apply(len)\n",
        "\n",
        "# ðŸŸ¢ **Integration Start: Additional Food Words + Extra Numeric Features**\n",
        "\n",
        "# 4.1. Define an Expanded List of Food-Related Words (Turkish & English)\n",
        "food_words = [\n",
        "    # Turkish Food Words\n",
        "    'elma', 'ekmek', 'et', 'balÄ±k', 'peynir', 'Ã§orba', 'salata', 'kahve', 'Ã§ay',\n",
        "    'tavuk', 'pirinÃ§', 'makarna', 'sÃ¼t', 'yumurta', 'sebze', 'meyve', 'biber',\n",
        "    'zeytin', 'hamsi', 'kÃ¶fte', 'kebab', 'dolma', 'lahana', 'patates', 'soÄŸan',\n",
        "    'domates', 'salÃ§a', 'biberiye', 'tarÃ§Ä±n', 'ÅŸeker', 'tuz', 'baharat', 'meyve suyu',\n",
        "    'fÄ±rÄ±n', 'bÃ¶rek', 'tatlÄ±', 'dondurma', 'pizza', 'hamburger', 'sosis', 'dÃ¶ner',\n",
        "    'sÃ¼tlÃ¼ tatlÄ±', 'tiramisu', 'tarif', 'baliktarifi', 'tarifini', 'tariflerimiz', 'tarifimizle',\n",
        "    'baklava', 'lokma', 'mÃ¼cver', 'gÃ¶zleme', 'Ã§Ä±lbÄ±r', 'ÅŸiÅŸ', 'lahmacun', 'cacÄ±k',\n",
        "    'midye', 'kÃ¼nefe', 'kadayÄ±f', 'lokum', 'bÃ¼ryan', 'tantuni', 'menemen',\n",
        "\n",
        "    # English Food Words\n",
        "    'apple', 'bread', 'meat', 'fish', 'cheese', 'soup', 'salad', 'coffee', 'tea',\n",
        "    'chicken', 'rice', 'pasta', 'milk', 'egg', 'vegetable', 'fruit', 'pepper',\n",
        "    'olive', 'herring', 'kofta', 'kebab', 'stuffed grape leaves', 'cabbage', 'potato', 'onion',\n",
        "    'tomato', 'paste', 'rosemary', 'cinnamon', 'sugar', 'salt', 'spice', 'fruit juice',\n",
        "    'oven', 'borek', 'dessert', 'ice cream', 'pizza', 'hamburger', 'sausage', 'doner',\n",
        "    'milk dessert', 'tiramisu', 'recipe', 'fishrecipe', 'recipei', 'recipes', 'recipewith',\n",
        "    'cupcake', 'brownie', 'pancake', 'waffle', 'bagel', 'sushi', 'taco', 'burrito',\n",
        "    'guacamole', 'shawarma', 'falafel', 'steak', 'lamb'\n",
        "]\n",
        "\n",
        "# 4.2. Additional Numeric Features: average likes & comments\n",
        "def mean_likes(posts):\n",
        "    # If each post has something like: post['edge_liked_by']['count']\n",
        "    if not isinstance(posts, list) or len(posts) == 0:\n",
        "        return 0\n",
        "    likes = []\n",
        "    for p in posts:\n",
        "        try:\n",
        "            likes.append(p['edge_liked_by']['count'])\n",
        "        except:\n",
        "            likes.append(0)\n",
        "    return np.mean(likes) if len(likes) > 0 else 0\n",
        "\n",
        "def mean_comments(posts):\n",
        "    # If each post has something like: post['edge_media_to_comment']['count']\n",
        "    if not isinstance(posts, list) or len(posts) == 0:\n",
        "        return 0\n",
        "    comments = []\n",
        "    for p in posts:\n",
        "        try:\n",
        "            comments.append(p['edge_media_to_comment']['count'])\n",
        "        except:\n",
        "            comments.append(0)\n",
        "    return np.mean(comments) if len(comments) > 0 else 0\n",
        "\n",
        "merged_data['like_mean'] = merged_data['posts'].apply(mean_likes)\n",
        "merged_data['comment_mean'] = merged_data['posts'].apply(mean_comments)\n",
        "\n",
        "# 4.3. Create 'food_word_count'\n",
        "merged_data['food_word_count'] = merged_data['posts'].apply(lambda x: count_food_words_exact(x, food_words))\n",
        "\n",
        "# ðŸŸ¢ **Integration End**\n",
        "\n",
        "print(\"\\nSample Data with 'food_word_count', 'like_mean', 'comment_mean':\")\n",
        "print(merged_data[['username', 'food_word_count', 'like_mean', 'comment_mean']].head())\n",
        "\n",
        "# 5. Prepare Features (X) and Target (y) for Binary Classification\n",
        "merged_data['binary_label'] = merged_data['label'].apply(lambda x: 1 if x == category_mapping['Food'] else 0)\n",
        "\n",
        "# Combine all captions into a single text per user for TF-IDF\n",
        "merged_data['all_captions'] = merged_data['posts'].apply(\n",
        "    lambda posts: ' '.join([post.get('caption', '') for post in posts if isinstance(post.get('caption', ''), str)])\n",
        ")\n",
        "merged_data['all_captions'] = merged_data['all_captions'].apply(preprocess_caption)\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
        "tfidf_matrix = tfidf.fit_transform(merged_data['all_captions'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out(), index=merged_data.index)\n",
        "\n",
        "# Combine numeric + TF-IDF features\n",
        "X = pd.concat([\n",
        "    merged_data[['followers_count', 'following_count', 'media_count', 'num_posts',\n",
        "                 'food_word_count', 'like_mean', 'comment_mean']],  # Enhanced numeric features\n",
        "    tfidf_df\n",
        "], axis=1)\n",
        "y = merged_data['binary_label']\n",
        "\n",
        "# 6. Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "numeric_features = [\n",
        "    'followers_count', 'following_count', 'media_count', 'num_posts',\n",
        "    'food_word_count', 'like_mean', 'comment_mean'\n",
        "]\n",
        "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 7. Train-Test Split (Fixed with test-classification-round2.dat)\n",
        "# --------------------------------------------------------------------\n",
        "test_usernames = pd.read_csv(test_path, header=None)[0].tolist()\n",
        "\n",
        "test_data = merged_data[merged_data['username'].isin(test_usernames)].copy()\n",
        "train_data = merged_data[~merged_data['username'].isin(test_usernames)].copy()\n",
        "\n",
        "# Sort test data by the order in test_usernames\n",
        "test_data['username'] = pd.Categorical(test_data['username'], categories=test_usernames, ordered=True)\n",
        "test_data.sort_values('username', inplace=True)\n",
        "\n",
        "# Split the feature matrices\n",
        "X_train = X.loc[train_data.index]\n",
        "y_train = y.loc[train_data.index]\n",
        "\n",
        "X_test = X.loc[test_data.index]\n",
        "y_test = y.loc[test_data.index]\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 8. RandomizedSearchCV for Hyperparameter Tuning\n",
        "# --------------------------------------------------------------------\n",
        "param_dist = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': [None] + list(range(10, 51, 10)),\n",
        "    'min_samples_split': randint(2, 16),\n",
        "    'min_samples_leaf': randint(1, 7),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=100,  # Number of parameter settings to sample\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters from RandomizedSearchCV:\", random_search.best_params_)\n",
        "print(\"Best CV Score from RandomizedSearchCV:\", random_search.best_score_)\n",
        "\n",
        "# 9. Use the best estimator\n",
        "best_model = random_search.best_estimator_\n",
        "print(\"\\nModel training with best parameters (RandomizedSearchCV) completed.\")\n",
        "\n",
        "# 10. Make Predictions\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "\n",
        "# 11. Evaluate the Model\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
        "print(classification_report(y_test, y_pred_test, target_names=['Not Food', 'Food']))\n",
        "\n",
        "# 12. Prepare Output (Binary Classification)\n",
        "usernames_test = test_data['username'].tolist()  # Preserves test file order\n",
        "predicted_labels = ['Food' if label == 1 else 'Not Food' for label in y_pred_test]\n",
        "\n",
        "output = {\n",
        "    str(usernames_test[i]): str(predicted_labels[i]) for i in range(len(usernames_test))\n",
        "}\n",
        "\n",
        "# Optional: Save the output\n",
        "with open('predictions.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"\\nPredictions saved to 'predictions.json'.\")"
      ]
    }
  ]
}